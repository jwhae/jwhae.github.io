<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AI | Optimizers</title>
  <meta name="description" content="Learn about optimizers used in AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="How to configure Chalk">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://jwhae.github.io/posts/overview-optimizers ">
  <meta property="og:description" content="Overview of Deep Learning Optimizers">
  <meta property="og:site_name" content="Chalk">
  
  <meta property="og:image" content="http://jwhae.github.io/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
  <meta name="twitter:image" content="http://jwhae.github.io/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">

  <link href="http://jwhae.github.io/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />
  <link rel="icon" type="image/x-icon" href="/assets/favicon-dark-11327753546b2135c989eee5cd83497a2734b702928d016839d795f6c706e3d5.ico">
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon-dark-d161409442b7e523089f24d08d0a55951549ece7504207c376d53b020713494d.png">
  <link rel="stylesheet" type="text/css" href="/assets/dark-831218bc9e41aef39ee6a0bae4501195bccafcc13101ae2b9cd20493a6ec04c0.css">
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav scrollappear">
          <a href="/" class="header-logo" title="AI : Optmization">Optimizers</a>
            <ul class="header-links">
              <li>
                <a href="/about" title="About me">
                  <svg xmlns="http://www.w3.org/2000/svg" class="icon-about">
                    <use href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about" xlink:href="/assets/about-ecf154b571ab8034ae00aeed91a3b7ad68db80b46d958753ad6216c919486e88.svg#icon-about"></use>
                  </svg>
                </a>
              </li>
              <li>
                <a href="https://github.com/jwhae" rel="noreferrer noopener" target="_blank" title="GitHub">
                  <svg xmlns="http://www.w3.org/2000/svg" class="icon-github">
                    <use href="/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github" xlink:href="/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github"></use>
                  </svg>
                </a>
              </li>
              <li>
                <a href="/feed.xml" rel="noreferrer noopener" target="_blank" title="RSS">
                  <svg xmlns="http://www.w3.org/2000/svg" class="icon-rss">
                    <use href="/assets/rss-541ec5cea9cefd10d2fcfec01888f3f231a8829940249835fa7b7b3a12ae0d0d.svg#icon-rss" xlink:href="/assets/rss-541ec5cea9cefd10d2fcfec01888f3f231a8829940249835fa7b7b3a12ae0d0d.svg#icon-rss"></use>
                  </svg>
                </a>
              </li>
          </ul>
        </nav>
        
        <!-- Content of the blog post starts here -->
        <article class="article scrollappear">
          <header class="article-header">
            <h1>Overview of AI Optimizers</h1>
            <p>The main purpose of training a AI model is to identify parameters that minimizes the result of a <code class="highlighter-rouge">loss function</code>. 
               This can also be defined as a <code class="highlighter-rouge">optimization</code> of parameters in AI model. 
            </p>
            <div class="article-list-footer">
              <span class="article-list-date"> November 08, 2020 </span>
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                <a href="/tag/web" title="See all posts with tag 'AI'">AI</a>
                <a href="/tag/jekyll" title="See all posts with tag 'Android'">Android</a>
              </div>
            </div>
          </header>

          <div class="article-content">
            <p>There are a few optimization methods implemented in the field of deep learning</p>
            
            <h3>SGD</h3>
            <p>One of simpler optimizer to be implemented. It is better than vanila gradient descent</p>
            <code class="highlighter-rouge">W ← W - η * ∂L/∂W</code>
            <div class="article-list-footer">
              <span class="article-list-date">W : weights to be updated</span> 
              <span class="article-list-date">∂L/∂W : gradient of the loss function of W</span>             
              <span class="article-list-date">η : learning rate</span>
              <span class="article-list-date">v : velocity</span>>
            </div>
            
            <figure class="highlight">
              <!-- 
                code block 
                watch out for identation, the code block require each of the line to start from the left corner  
              -->
              <pre><code class="language-html" data-lang="html"><span class="c">&lt;-- SGD --&gt;</span>
<span class="na">class</span> SGD :
    <span class="o">def</span> __init__(self, lr= 0.01) : 
        <span class="k">self</span>.lr = lr
                
    <span class="o">def</span> update(self, params, grads):
        <span class="l">for</span> key <span class="l">in</span> params.keys():
            params[key] -= <span class="k">self</span>.lr * grads[key]
    </code></pre></figure>
            <p>Despite its simplicity, SGD has some problems when implemented</p>
            <ul>
              <li><code class="highlighter-rouge">convergence</code>: as it updates weight using its derivatives, if the derivative is one, there is no more update of the weight, thereby stuck in the local optimal(sub-optimal).</li>
              <li><code class="highlighter-rouge">efficiency</code>: for anistropy functions, which the gradient changes according its direction, the changes to the weight is applied too radically, requiring more time to converge. </li>
              <li><code class="highlighter-rouge">learning rate</code>: without appropriate learning rate, learning does occur at a optimal level. Too big of l.r causes learning to fail, and too little of l.r causes learning to take to converge. </li>
            </ul>
            
            <h3>Momentum</h3>
            <p>To solve the <code class="highlighter-rouge">convergence</code> that SGD has, momentum optimization was introduced</p>
            <p>By considering, the velocity of the gradient, it updates the gradient even when it is a local gradient, and also updating the gradient faster making it more efficient</p>
            
            <code class="highlighter-rouge">v ← αv - η * ∂L/∂W</code>
            <code class="highlighter-rouge">W ← W + v</code>
            
            <div class="article-list-footer">
              <span class="article-list-date">W : weights to be updated</span> 
              <span class="article-list-date">\n∂L/∂W : gradient of the loss function of W</span>             
              <span class="article-list-date">\nη : learning rate</span>
              <span class="article-list-date">\nv : velocity</span>
              <span class="article-list-date">\nα : friction (0.9) </span>
            </div>
            
            <figure class="highlight">
              <!-- 
                code block 
                watch out for identation, the code block require each of the line to start from the left corner  
              -->
              <pre><code class="language-html" data-lang="html"><span class="c">&lt;-- Momentum --&gt;</span>
<span class="na">class</span> Momentum :
    <span class="o">def</span> __init__(self, lr= 0.01, momentum = 0.9) : 
        <span class="k">self</span>.lr = lr
        <span class="k">self</span>.momentum = momentum
        <span class="k">self</span>.v = <span class="o">None</span>
                
    <span class="o">def</span> update(self, params, grads):
        <span class="l">if</span> <span class="k">self</span>.h <span class="l">is</span> <span class="o">None</span>:
            <span class="k">self</span>.v = {}
            
            <span class="l">for</span> key, val <span class="l">in</span> params.items():
                <span class="k">self</span>.v[key] = np.zeros_like(val)
               
        <span class="l">for</span> key <span class="l">in</span> params.keys():
            <span class="k">self</span>.v[key] = <span class="k">self</span>.momentum * <span class="k">self</span>.v[key] - <span class="k">self</span>.lr * grads[key]
            params[key] += <span class="k">self</span>.v[key]
    </code></pre></figure>
            
            <h3>Nesterov</h3>
            <p>Improved version of momentum optimizer</p>
            <figure class="highlight">
            <pre><code class="language-html" data-lang="html"><span class="c">&lt;-- Nestrov --&gt;</span>
<span class="na">class</span> Adam :
    <span class="o">def</span> __init__(self, momentum = 0.9) : 
        <span class="k">self</span>.lr = lr
        <span class="k">self</span>.momentum = momentum
        <span class="k">self</span>.v = <span class="o">None</span>
                
    <span class="o">def</span> update(self, params, grads):
        <span class="l">if</span> <span class="k">self</span>.h <span class="l">is</span> <span class="o">None</span>:
            <span class="k">self</span>.v = {}
            
            <span class="l">for</span> key, val <span class="l">in</span> params.items():
                <span class="k">self</span>.v[key] = np.zeros_like(val)
                
        <span class="l">for</span> key <span class="l">in</span> params.keys():
            <span class="k">self</span>.v[key] *= <span class="k">self</span>.momentum
            <span class="k">self</span>.v[key] -= <span class="k">self</span>.lr * grads[key]
            params[key] += <span class="k">self</span>.momentum ** 2 * <span class="k">self</span>.v[key]
            params[key] -= (1 + <span class="k">self</span>.momentum) * <span class="k">self</span>.lr * grads[param]
    </code></pre></figure>

            <h3>
              AdaGrad</h3>
            <p>To solve the <code class="highlighter-rouge">learning rate</code> problem SGD has, AdaGrad optimization method was introduced</p>
            <p>Initially there was <code class="highlighter-rouge">learning rate decay</code>, which decreased the learning rate as the learning progresses</p>
            <p>However, in learning rate decay, paramters were all decreased at an identical speed</p>
            
            <p>In <code class="highlighter-rouge">AdaGrad</code> improved learning rate decay by adaptively updating the degree of decay for each parameters in the model</p>
            <p>However, even in Adagrad, there is problem that the magnitude of the update gradually becomes weak as learning progresses, as it sums two square of all past gradients</p>
            <p>To solve this, <code class="highlighter-rouge">RMSProp</code> was introduced. It allows gradient in the far past to be forgotten and the recent gradient to be applied with high significance (Exponential Moving Average)</p>
            
             <code class="highlighter-rouge">h ← h + ∂L/∂W ⊙ ∂L/∂W</code>
            <code class="highlighter-rouge">W ← W - η *1/√h * ∂L/∂W</code>
            <div class="article-list-footer">
              <span class="article-list-date">W : weights to be updated</span> 
              <span class="article-list-date">∂L/∂W : gradient of the loss function of W</span>             
              <span class="article-list-date">η : learning rate</span>
              <span class="article-list-date">h : sum of two square of the previous gradients</span>>
            </div>
            
            <figure class="highlight">
            <pre><code class="language-html" data-lang="html"><span class="c">&lt;-- AdaGrad --&gt;</span>
<span class="na">class</span> AdaGrad :
    <span class="o">def</span> __init__(self, lr= 0.01) : 
        <span class="k">self</span>.lr = lr
        <span class="k">self</span>.h = <span class="o">None</span>
                
    <span class="o">def</span> update(self, params, grads):
        <span class="l">if</span> <span class="k">self</span>.h <span class="l">is</span> <span class="o">None</span>:
            <span class="k">self</span>.h = {}
            
            <span class="l">for</span> key, val <span class="l">in</span> params.items():
                <span class="k">self</span>.h[key] = np.zeros_like(val)
               
        <span class="l">for</span> key <span class="l">in</span> params.keys():
            <span class="k">self</span>.h[key] = grads[key]**2
            params[key] -= <span class="k">self</span>.lr * grads[key] / (np.sqrt(<span class="k">self</span>.h[key] + 1e-7)
    </code></pre></figure>
  
            <h3>
              RMSProp (Root Mean Square Propagation)</h3>
            <figure class="highlight">
            <pre><code class="language-html" data-lang="html"><span class="c">&lt;-- RMSprop --&gt;</span>
<span class="na">class</span> RMSprop :
    <span class="o">def</span> __init__(self, lr= 0.01, decay_rate = 0.99) : 
        <span class="k">self</span>.lr = lr
        <span class="k">self</span>.decay_rate = decay_rate
        <span class="k">self</span>.h = <span class="o">None</span>
                
    <span class="o">def</span> update(self, params, grads):
        <span class="l">if</span> <span class="k">self</span>.h <span class="l">is</span> <span class="o">None</span>:
            <span class="k">self</span>.h = {}
            
            <span class="l">for</span> key, val <span class="l">in</span> params.items():
                <span class="k">self</span>.h[key] = np.zeros_like(val)
                
        <span class="l">for</span> key <span class="l">in</span> params.keys():
            <span class="k">self</span>.h[key] *= <span class="k">self</span>.decay_rate
            <span class="k">self</span>.h[key] += (1 - <span class="k">self</span>.decay_rate) * grads[key]**2
            
            params[key] -= <span class="k">self</span>.lr * grads[key] / (np.sqrt(<span class="k">self</span>.h[key] + 1e-7)
            
    </code></pre></figure>
  
<h3>
  Adam optimization</h3>
            <p>It's an optimization technique where it combined both the momentum and Adagrad</p>
            <p>Also it allows <a href="https://hansololee.github.io/optimization/adam/" title ="bias correction"><code class="highlighter-rouge">bias correction</code></a> of the hyper parameters</p>
            <figure class="highlight">
            <pre><code class="language-html" data-lang="html"><span class="c">&lt;-- Adam --&gt;</span>
<span class="na">class</span> Adam :
    <span class="o">def</span> __init__(self, lr= 0.01, beta1 = 0.9 beta2 = 0.999) : 
        <span class="k">self</span>.lr = lr
        <span class="k">self</span>.beta1 = beta1
        <span class="k">self</span>.beta2 = beta2
        <span class="k">self</span>.iter = 0
        <span class="k">self</span>.m = <span class="o">None</span>
        <span class="k">self</span>.v = <span class="o">None</span>
                
    <span class="o">def</span> update(self, params, grads):
        <span class="l">if</span> <span class="k">self</span>.h <span class="l">is</span> <span class="o">None</span>:
            <span class="k">self</span>.m = {}
            
            <span class="l">for</span> key, val <span class="l">in</span> params.items():
                <span class="k">self</span>.m[key] = np.zeros_like(val)
                <span class="k">self</span>.v[key] = np.zeros_like(val)
                
        <span class="k">self</span>.iter += 1 
        lr_t = <span class="k">self</span>.lr * np.sqrt(1.0 - <span class="k">self</span>.beta2 ** <span class="k">self</span>.iter) / (1.0 - <span class="k">self</span>.beta1 ** <span class="k">self</span>.iter)
               
        <span class="l">for</span> key <span class="l">in</span> params.keys():
            <span class="k">self</span>.m[key] += (1 - <span class="k">self</span>.beta1) * (grads[key] - <span class="k">self</span>.m[key])
            <span class="k">self</span>.v[key] += (1 - <span class="k">self</span>.beta2) * (grads[key]**2 - <span class="k">self</span>.v[key])
            
            params[key] -= lr_t * <span class="k">self</span>.m[key] / (np.sqrt(<span class="k">self</span>.v[key] + 1e-7)
            
    </code></pre></figure>


            
        <footer class="footer scrollappear">
          <p>
            Chalk is a high quality, completely customizable, performant and 100% free
            blog template for Jekyll built by
            <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
          </p>
        </footer>
      </div>
    </div>
  </main>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-28631876-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-28631876-6');
  </script>

<script src="/assets/vendor-130c9c254effc51f3283620bc635851da7b99c20901216948f11ba72ee13317f.js" type="text/javascript"></script>
<script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>
<script src="/assets/scrollappear-e2da8ea567e418637e31266cc5302126eaa79f62a2273739086358b589a89ee6.js" type="text/javascript"></script>
<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


</body>
</html>
